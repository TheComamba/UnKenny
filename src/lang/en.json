{
    "unkenny.confirmation.title": "Confirmation",
    "unkenny.confirmation.question": "Are you sure?",
    "unkenny.confirmation.yes": "Yes",
    "unkenny.confirmation.no": "No",
    "unkenny.confirmation.confirmed": "Confirmed",
    "unkenny.confirmation.cancelled": "Cancelled",
    "unkenny.chatLog.clearConversations": "Clear UnKenny Conversations",
    "unkenny.chatLog.clearConversationsConfirmation": "Are you sure you want to clear all UnKenny Conversations? This will not delete any messages.",
    "unkenny.chatLog.removeFromConversation": "Remove from UnKenny Conversation",
    "unkenny.chatMessage.actorIdNotFound": "Actor with id \"{id}\" not found. Has it been deleted?",
    "unkenny.chatMessage.actorAliasNotFound": "Actor with alias \"{alias}\" not found.",
    "unkenny.chatMessage.marker": "Speaking with {audience}",
    "unkenny.chatMessage.multipleActors": "Multiple actors with alias \"{alias}\" found: {names}",
    "unkenny.chatMessage.triggerWithoutActor": "triggerResponse was called with no actor.",
    "unkenny.chatMessage.noResponse": "No response was generated.",
    "unkenny.chatMessage.jsonParseError": "Error parsing JSON: {error}",
    "unkenny.chatMessage.preambleTooLong": "The conversation has only just begun, but it is already too long for the model. This is likely due to the preamble being too long. Please shorten the preamble or switch to a different model with a larger context size.",
    "unkenny.chatMessage.truncatingMessage": "This conversion spanning {messageCount} messages is too long for the model, and will be truncated. To prevent this in the future, you can either switch the model or shorten the conversation by deleting previous messages.",
    "unkenny.chatMessage.noPreamble": "No preamble set for actor {name}.",
    "unkenny.info.title": "Please be patient",
    "unkenny.info.message": "This may take a while, during which FoundryVTT will be unresponsive.",
    "unkenny.llm.noValue": "No value found for {parameterName}. Using default value {value}.",
    "unkenny.llm.noDefaultValue": "No default value found for {parameterName}.",
    "unkenny.llm.preparingModel": "Preparing model \"{model}\"...",
    "unkenny.llm.preparingTokenizer": "Preparing tokenizer for \"{model}\"...",
    "unkenny.llm.generatingResponse": "Generating {actorName}'s response...",
    "unkenny.llm.localLlmError": "An error occurred during text generation with local LLM: {error}",
    "unkenny.llm.openAiError": "Unable to get response from OpenAI: {error}",
    "unkenny.settings.model": "Large Language Model",
    "unkenny.settings.modelDescription": "The default model used by unkenny actors to generate responses.\\Local models run in your browser or FoundryVTT instance, while OpenAI models run on a remote server.\\OpenAI models are much faster and mostly yield better results, but they require an API key to work.",
    "unkenny.settings.apiKey": "OpenAI API Key",
    "unkenny.settings.apiKeyDescription": "If you want to use OpenAI models, you need to provide an API key here.\\Additionally, your account must have a positive balance.",
    "unkenny.settings.minNewTokens": "Minimum Number of New Tokens",
    "unkenny.settings.minNewTokensDescription": "In large language models, the number of tokens determines the length of the generated text.\\To avoid very short responses, you can set a minimum number of tokens here. Note: This parameter is only considered in local models.",
    "unkenny.settings.maxNewTokens": "Maximum Number of New Tokens",
    "unkenny.settings.maxNewTokensDescription": "To avoid overly long responses, you can set a maximum number of tokens here.",
    "unkenny.settings.repetitionPenalty": "Repetition Penalty / Frequency Penalty",
    "unkenny.settings.repetitionPenaltyDescription": "The repetition penalty is a number that makes it less likely for a token that has already been generated to be generated again.\\Higher values reduce the likelihood of repetition, negative values increase it.",
    "unkenny.settings.temperature": "Temperature",
    "unkenny.settings.temperatureDescription": "Large language models generate text by sampling from a probability distribution over the vocabulary.\\Temperature infuences this distribution:\\With a temperature of 0, the model always chooses the most likely token, while all token become nearly equally likely for very high temperatures.\\Lower values make the model more conservative, higher values make it more creative.",
    "unkenny.settings.prefixWithTalk": "Prefix responses with /talk",
    "unkenny.settings.prefixWithTalkDescription": "If this option is checked and the Talking Actors FoundryVTT module is enabled and set up, the model's responses are read out loud by an AI voice.",
    "unkenny.shared.unkenninessForNull": "Unkenniness checked for null actor.",
    "unkenny.shared.moduleLoadFailed": "Unable to load module \"{name}\": {error}",
    "unkenny.sheet.title": "Edit UnKennyness Parameters",
    "unkenny.sheet.alias": "Alias",
    "unkenny.sheet.preamble": "Preamble",
    "unkenny.sheet.overwrite": "Overwrite Global Parameters",
    "unkenny.sheet.model": "Model",
    "unkenny.sheet.minTokens": "Minimum number of generated tokens",
    "unkenny.sheet.maxTokens": "Maximum number of generated tokens",
    "unkenny.sheet.repetitionPenalty": "Repetition penalty",
    "unkenny.sheet.temperature": "Temperature",
    "unkenny.sheet.save": "Save",
    "unkenny.sheet.settingAliasFailed": "Actor not found, setting alias failed."
}